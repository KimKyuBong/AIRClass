"""
AIRClass NLP Module
ì±„íŒ… ë° ìŒì„± ë‚´ìš©ì˜ ìì—°ì–´ ì²˜ë¦¬
"""

import logging
import json
from datetime import datetime
from typing import Optional, Dict, List, Tuple
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class SentimentType(str, Enum):
    """ê°ì • íƒ€ì…"""

    POSITIVE = "positive"
    NEUTRAL = "neutral"
    NEGATIVE = "negative"
    CONFUSED = "confused"


class IntentType(str, Enum):
    """ì‚¬ìš©ì ì˜ë„"""

    QUESTION = "question"
    ANSWER = "answer"
    CLARIFICATION = "clarification"
    OPINION = "opinion"
    GREETING = "greeting"
    FEEDBACK = "feedback"
    OTHER = "other"


@dataclass
class TokenAnalysis:
    """í† í° ë¶„ì„ ê²°ê³¼"""

    token: str
    part_of_speech: str  # "NOUN", "VERB", "ADJ", etc.
    lemma: str
    entity_type: Optional[str] = None  # NER: "PERSON", "ORG", "CONCEPT", etc.


@dataclass
class ChatMessage:
    """ì±„íŒ… ë©”ì‹œì§€ ë¶„ì„"""

    message_id: str
    session_id: str
    user_id: str
    user_type: str  # "teacher", "student"
    content: str
    timestamp: str

    # ê°ì • ë¶„ì„
    sentiment: SentimentType
    sentiment_score: float  # -1.0 (negative) to 1.0 (positive)

    # ì˜ë„ ë¶„ì„
    intent: IntentType
    intent_confidence: float  # 0.0-1.0

    # ì–¸ì–´ ë¶„ì„
    language: str  # "ko", "en", etc.
    tokens: List[TokenAnalysis]
    keywords: List[str]  # ì£¼ìš” í‚¤ì›Œë“œ
    entities: List[Dict]  # ëª…ëª…ëœ ì—”í‹°í‹°

    # êµìœ¡ì  ë¶„ì„
    learning_indicator: Optional[
        str
    ]  # "understands", "confused", "asking_deep_question"
    topic_relevance: float  # 0.0-1.0
    response_required: bool  # êµì‚¬ ì‘ë‹µ í•„ìš” ì—¬ë¶€

    # ë©”íƒ€ë°ì´í„°
    quality_score: float  # 0.0-1.0 (í•™ìŠµì— ë„ì›€ì´ ë˜ëŠ” ì •ë„)


@dataclass
class ConversationSummary:
    """ëŒ€í™” ìš”ì•½"""

    session_id: str
    summary: str
    main_topics: List[str]
    key_questions: List[str]
    student_understanding_level: str  # "excellent", "good", "fair", "poor"
    engagement_level: str  # "high", "medium", "low"
    areas_needing_review: List[str]


class NLPAnalyzer:
    """ìì—°ì–´ ì²˜ë¦¬ ë¶„ì„ê¸°"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.message_cache: Dict[str, ChatMessage] = {}
        self.conversation_history: Dict[str, List[ChatMessage]] = {}

        # í‚¤ì›Œë“œ ë°ì´í„°ë² ì´ìŠ¤ (ì‹¤ì œë¡œëŠ” ë” í¼)
        self.keywords_by_topic = {
            "functions": ["def", "í•¨ìˆ˜", "function", "parameter", "return", "í˜¸ì¶œ"],
            "data_structures": ["list", "dict", "array", "ë°°ì—´", "ìë£Œêµ¬ì¡°", "ë°°ì—´"],
            "algorithms": [
                "algorithm",
                "ì•Œê³ ë¦¬ì¦˜",
                "loop",
                "ë°˜ë³µ",
                "recursive",
                "ì •ë ¬",
            ],
            "web": ["html", "css", "javascript", "ì›¹", "ì„œë²„", "í´ë¼ì´ì–¸íŠ¸"],
            "database": ["database", "sql", "mongodb", "ë°ì´í„°ë² ì´ìŠ¤", "ì¿¼ë¦¬"],
        }

        logger.info("ğŸ—£ï¸ NLPAnalyzer initialized")

    def analyze_message(
        self,
        session_id: str,
        message_id: str,
        user_id: str,
        user_type: str,
        content: str,
    ) -> ChatMessage:
        """
        ì±„íŒ… ë©”ì‹œì§€ ë¶„ì„

        Args:
            session_id: ì„¸ì…˜ ID
            message_id: ë©”ì‹œì§€ ID
            user_id: ì‚¬ìš©ì ID
            user_type: ì‚¬ìš©ì íƒ€ì… (teacher, student)
            content: ë©”ì‹œì§€ ë‚´ìš©

        Returns:
            ChatMessage: ë¶„ì„ëœ ë©”ì‹œì§€
        """
        try:
            # 1. ê¸°ë³¸ ì²˜ë¦¬
            content_lower = content.lower()

            # 2. ê°ì • ë¶„ì„
            sentiment, sentiment_score = self._analyze_sentiment(content)

            # 3. ì˜ë„ ë¶„ì„
            intent, intent_confidence = self._analyze_intent(content)

            # 4. í† í°í™” ë° ë¶„ì„
            tokens = self._tokenize_and_analyze(content)

            # 5. í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(tokens, content)

            # 6. ì—”í‹°í‹° ì¸ì‹
            entities = self._extract_entities(tokens)

            # 7. ì–¸ì–´ ê°ì§€
            language = self._detect_language(content)

            # 8. í•™ìŠµ ì§€í‘œ ë¶„ì„
            learning_indicator = self._identify_learning_indicator(
                content, sentiment, intent
            )

            # 9. ì£¼ì œ ê´€ë ¨ì„± ê³„ì‚°
            topic_relevance = self._calculate_topic_relevance(keywords, content)

            # 10. êµì‚¬ ì‘ë‹µ í•„ìš” ì—¬ë¶€
            response_required = self._evaluate_response_required(
                intent, user_type, sentiment
            )

            # 11. í’ˆì§ˆ ì ìˆ˜
            quality_score = self._calculate_quality_score(
                intent, sentiment_score, topic_relevance
            )

            # ë©”ì‹œì§€ ìƒì„±
            message = ChatMessage(
                message_id=message_id,
                session_id=session_id,
                user_id=user_id,
                user_type=user_type,
                content=content,
                timestamp=datetime.now().isoformat(),
                sentiment=sentiment,
                sentiment_score=sentiment_score,
                intent=intent,
                intent_confidence=intent_confidence,
                language=language,
                tokens=tokens,
                keywords=keywords,
                entities=entities,
                learning_indicator=learning_indicator,
                topic_relevance=topic_relevance,
                response_required=response_required,
                quality_score=quality_score,
            )

            # ìºì‹œì— ì €ì¥
            self.message_cache[message_id] = message

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
            if session_id not in self.conversation_history:
                self.conversation_history[session_id] = []
            self.conversation_history[session_id].append(message)

            logger.info(f"âœ… Message analyzed: {message_id}")
            return message

        except Exception as e:
            logger.error(f"âŒ Failed to analyze message: {e}")
            raise

    def _analyze_sentiment(self, content: str) -> Tuple[SentimentType, float]:
        """ê°ì • ë¶„ì„"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ê°ì • ë¶„ì„ ëª¨ë¸ ì‚¬ìš© (VADER, BERT ë“±)
            content_lower = content.lower()

            positive_words = [
                "ì¢‹ë‹¤",
                "ì¢‹ì•„",
                "í›Œë¥­í•˜ë‹¤",
                "excellent",
                "great",
                "good",
                "ì™„ë²½",
                "ì¢‹ì€",
                "ê°ì‚¬",
            ]
            negative_words = [
                "ì‹«ë‹¤",
                "ì‹«ì–´",
                "ë‚˜ì˜ë‹¤",
                "ëª»",
                "ì•ˆ",
                "ì–´ë µë‹¤",
                "ëª¨ë¥´ê² ë‹¤",
                "ì´ìƒí•˜ë‹¤",
            ]
            confused_words = [
                "ë­",
                "ë­”ë°",
                "ë­ì§€",
                "ì´ê²Œ",
                "ëª¨ë¥´",
                "ì´í•´",
                "ì–´ë–»ê²Œ",
                "ì™œ",
            ]

            pos_count = sum(1 for word in positive_words if word in content_lower)
            neg_count = sum(1 for word in negative_words if word in content_lower)
            conf_count = sum(1 for word in confused_words if word in content_lower)

            if conf_count > 0:
                return SentimentType.CONFUSED, -0.3
            elif pos_count > neg_count:
                score = min(0.3 + (pos_count * 0.1), 1.0)
                return SentimentType.POSITIVE, score
            elif neg_count > pos_count:
                score = max(-0.3 - (neg_count * 0.1), -1.0)
                return SentimentType.NEGATIVE, score
            else:
                return SentimentType.NEUTRAL, 0.0

        except Exception as e:
            logger.warning(f"âš ï¸ Sentiment analysis failed: {e}")
            return SentimentType.NEUTRAL, 0.0

    def _analyze_intent(self, content: str) -> Tuple[IntentType, float]:
        """ì˜ë„ ë¶„ì„"""
        try:
            content_lower = content.lower()

            # ì§ˆë¬¸ ê°ì§€
            if "?" in content or any(
                w in content_lower
                for w in ["ë­", "ë­ì•¼", "ë¬´ì—‡", "ì–´ë–»ê²Œ", "ì™œ", "ì–¸ì œ", "ì–´ë””", "ëˆ„ê°€"]
            ):
                return IntentType.QUESTION, 0.9

            # ë‹µë³€ íŒ¨í„´ (ìˆ«ì, ì˜ˆ/ì•„ë‹ˆì˜¤ ë“±)
            if any(
                w in content_lower
                for w in ["ë„¤", "ì•„ë‹ˆìš”", "ë§ë‹¤", "ê·¸ë ‡ë‹¤", "ë‹µ", "ì •ë‹µ"]
            ):
                return IntentType.ANSWER, 0.85

            # ëª…í™•íˆ ìš”ì²­ ("ë‹¤ì‹œ ì„¤ëª…í•´ì£¼ì„¸ìš”")
            if any(
                w in content_lower for w in ["ë‹¤ì‹œ", "ì„¤ëª…", "ëª…í™•", "ë‹¤ì‹œ", "ì„¤ëª…í•´"]
            ):
                return IntentType.CLARIFICATION, 0.8

            # ì˜ê²¬ ("ìƒê°", "ì˜ê²¬", "ë‚˜ëŠ”")
            if any(
                w in content_lower for w in ["ìƒê°", "ì˜ê²¬", "ë‚˜ëŠ”", "ë‚´ ìƒê°", "ì €ëŠ”"]
            ):
                return IntentType.OPINION, 0.75

            # ì¸ì‚¬ë§
            if any(
                w in content_lower
                for w in ["ì•ˆë…•", "ì•ˆë…•í•˜ì„¸ìš”", "hi", "hello", "ì¢‹ì•„"]
            ):
                return IntentType.GREETING, 0.95

            # í”¼ë“œë°±
            if any(
                w in content_lower for w in ["ì˜ê²¬", "ì œì•ˆ", "ì¢‹ë‹¤", "ì‹«ë‹¤", "í”¼ë“œë°±"]
            ):
                return IntentType.FEEDBACK, 0.7

            return IntentType.OTHER, 0.5

        except Exception as e:
            logger.warning(f"âš ï¸ Intent analysis failed: {e}")
            return IntentType.OTHER, 0.5

    def _tokenize_and_analyze(self, content: str) -> List[TokenAnalysis]:
        """í† í°í™” ë° ë¶„ì„"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” KoNLPy, spaCy ë“±ì„ ì‚¬ìš©
            # í˜„ì¬ëŠ” ê°„ë‹¨í•œ í† í¬ë‚˜ì´ì € ì‚¬ìš©
            tokens = content.split()
            analyzed = []

            for token in tokens[:20]:  # ì²˜ìŒ 20ê°œë§Œ
                analysis = TokenAnalysis(
                    token=token,
                    part_of_speech="NOUN",  # ëª¨ì˜ ê°’
                    lemma=token.lower(),
                    entity_type=None,
                )
                analyzed.append(analysis)

            return analyzed

        except Exception as e:
            logger.warning(f"âš ï¸ Tokenization failed: {e}")
            return []

    def _extract_keywords(self, tokens: List[TokenAnalysis], content: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            keywords = []
            content_lower = content.lower()

            # ì£¼ì œë³„ í‚¤ì›Œë“œ ë§¤ì¹­
            for topic, keywords_list in self.keywords_by_topic.items():
                for keyword in keywords_list:
                    if keyword.lower() in content_lower:
                        keywords.append(keyword)

            # ì¤‘ë³µ ì œê±° ë° ìƒìœ„ 5ê°œë§Œ
            return list(set(keywords))[:5]

        except Exception as e:
            logger.warning(f"âš ï¸ Keyword extraction failed: {e}")
            return []

    def _extract_entities(self, tokens: List[TokenAnalysis]) -> List[Dict]:
        """ì—”í‹°í‹° ì¶”ì¶œ (NER)"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” NER ëª¨ë¸ ì‚¬ìš©
            entities = []

            # ê°œë… ê¸°ë°˜ ì—”í‹°í‹° ê°ì§€ (ëª¨ì˜)
            concept_keywords = ["í•¨ìˆ˜", "ë°ì´í„°", "ì•Œê³ ë¦¬ì¦˜", "ë³€ìˆ˜"]

            for token in tokens:
                if token.token in concept_keywords:
                    entities.append(
                        {"text": token.token, "type": "CONCEPT", "confidence": 0.8}
                    )

            return entities

        except Exception as e:
            logger.warning(f"âš ï¸ Entity extraction failed: {e}")
            return []

    def _detect_language(self, content: str) -> str:
        """ì–¸ì–´ ê°ì§€"""
        try:
            # ê°„ë‹¨í•œ í•œê¸€ ê°ì§€
            korean_chars = sum(
                1 for c in content if ord(c) >= 0xAC00 and ord(c) <= 0xD7A3
            )
            if korean_chars > len(content) * 0.3:
                return "ko"
            return "en"

        except Exception as e:
            logger.warning(f"âš ï¸ Language detection failed: {e}")
            return "ko"  # ê¸°ë³¸ê°’

    def _identify_learning_indicator(
        self, content: str, sentiment: SentimentType, intent: IntentType
    ) -> Optional[str]:
        """í•™ìŠµ ì§€í‘œ ì‹ë³„"""
        try:
            content_lower = content.lower()

            # ì´í•´ë„ í™•ì¸
            if any(
                w in content_lower
                for w in ["ì•Œê² ìŠµë‹ˆë‹¤", "ì´í•´ë", "ì•Œì•˜ì–´", "ë§ë‹¤", "ê·¸ë ‡êµ°ìš”"]
            ):
                return "understands"

            # í˜¼ë€ë„ í™•ì¸
            if sentiment == SentimentType.CONFUSED or any(
                w in content_lower for w in ["ëª¨ë¥´ê² ë‹¤", "ì´í•´ê°€", "ë­ì§€", "ì„¤ëª…í•´"]
            ):
                return "confused"

            # ê¹Šì´ ìˆëŠ” ì§ˆë¬¸
            if intent == IntentType.QUESTION and any(
                w in content_lower for w in ["ì™œ", "ì–´ë–»ê²Œ", "ê·¸ëŸ¬ë©´"]
            ):
                return "asking_deep_question"

            return None

        except Exception as e:
            logger.warning(f"âš ï¸ Learning indicator identification failed: {e}")
            return None

    def _calculate_topic_relevance(self, keywords: List[str], content: str) -> float:
        """ì£¼ì œ ê´€ë ¨ì„± ê³„ì‚°"""
        try:
            if not keywords:
                return 0.5  # ì¤‘ë¦½

            # í‚¤ì›Œë“œ ê°œìˆ˜ ê¸°ë°˜ ì ìˆ˜
            score = min(len(keywords) * 0.25, 1.0)

            # í‚¤ì›Œë“œ ë°˜ë³µ íšŸìˆ˜ ê³ ë ¤
            content_lower = content.lower()
            for keyword in keywords:
                count = content_lower.count(keyword.lower())
                if count > 1:
                    score = min(score + 0.1, 1.0)

            return score

        except Exception as e:
            logger.warning(f"âš ï¸ Topic relevance calculation failed: {e}")
            return 0.5

    def _evaluate_response_required(
        self, intent: IntentType, user_type: str, sentiment: SentimentType
    ) -> bool:
        """êµì‚¬ ì‘ë‹µ í•„ìš” ì—¬ë¶€ í‰ê°€"""
        try:
            # í•™ìƒì˜ ì§ˆë¬¸ì€ í•­ìƒ ì‘ë‹µ í•„ìš”
            if user_type == "student" and intent == IntentType.QUESTION:
                return True

            # í˜¼ë€ë„ê°€ ë†’ìœ¼ë©´ ì‘ë‹µ í•„ìš”
            if sentiment == SentimentType.CONFUSED:
                return True

            # ê¹Šì´ ìˆëŠ” ì§ˆë¬¸
            if user_type == "student" and intent == IntentType.OPINION:
                return True

            return False

        except Exception as e:
            logger.warning(f"âš ï¸ Response evaluation failed: {e}")
            return False

    def _calculate_quality_score(
        self, intent: IntentType, sentiment_score: float, topic_relevance: float
    ) -> float:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            score = 0.0

            # ì˜ë„ ê¸°ë°˜ ì ìˆ˜
            intent_scores = {
                IntentType.QUESTION: 0.7,
                IntentType.ANSWER: 0.6,
                IntentType.CLARIFICATION: 0.8,
                IntentType.OPINION: 0.5,
                IntentType.GREETING: 0.2,
                IntentType.FEEDBACK: 0.5,
                IntentType.OTHER: 0.3,
            }
            score += intent_scores.get(intent, 0.3)

            # ê¸ì •ì  ê°ì • ì¶”ê°€ ì ìˆ˜
            if sentiment_score > 0.5:
                score += 0.15

            # ì£¼ì œ ê´€ë ¨ì„± ì¶”ê°€
            score += topic_relevance * 0.15

            return min(score, 1.0)

        except Exception as e:
            logger.warning(f"âš ï¸ Quality score calculation failed: {e}")
            return 0.5

    def summarize_conversation(self, session_id: str) -> ConversationSummary:
        """ëŒ€í™” ìš”ì•½"""
        try:
            messages = self.conversation_history.get(session_id, [])

            if not messages:
                raise ValueError(f"No conversation found for session: {session_id}")

            # ì£¼ìš” í† í”½ ì¶”ì¶œ
            all_keywords = []
            for msg in messages:
                all_keywords.extend(msg.keywords)
            main_topics = list(set(all_keywords))[:5]

            # ì£¼ìš” ì§ˆë¬¸ ì¶”ì¶œ
            key_questions = [
                msg.content
                for msg in messages
                if msg.intent == IntentType.QUESTION and len(msg.content) > 10
            ][:5]

            # í•™ìƒ ì´í•´ë„ í‰ê°€
            understanding_indicators = [
                msg.learning_indicator for msg in messages if msg.learning_indicator
            ]
            if "confused" in understanding_indicators:
                understanding = "poor"
            elif "asks_deep_question" in understanding_indicators:
                understanding = "excellent"
            elif "understands" in understanding_indicators:
                understanding = "good"
            else:
                understanding = "fair"

            # ì°¸ì—¬ë„ í‰ê°€
            student_messages = [m for m in messages if m.user_type == "student"]
            if len(student_messages) > len(messages) * 0.4:
                engagement = "high"
            elif len(student_messages) > len(messages) * 0.2:
                engagement = "medium"
            else:
                engagement = "low"

            # ë³µìŠµ í•„ìš” ì˜ì—­
            confused_messages = [
                m for m in messages if m.sentiment == SentimentType.CONFUSED
            ]
            areas_needing_review = []
            for msg in confused_messages:
                areas_needing_review.extend(msg.keywords)
            areas_needing_review = list(set(areas_needing_review))[:3]

            summary = ConversationSummary(
                session_id=session_id,
                summary=f"ì´ {len(messages)}ê°œì˜ ë©”ì‹œì§€ê°€ ìˆì—ˆê³ , ì£¼ìš” í† í”½ì€ {', '.join(main_topics)}ì…ë‹ˆë‹¤.",
                main_topics=main_topics,
                key_questions=key_questions,
                student_understanding_level=understanding,
                engagement_level=engagement,
                areas_needing_review=areas_needing_review,
            )

            logger.info(f"âœ… Conversation summarized: {session_id}")
            return summary

        except Exception as e:
            logger.error(f"âŒ Failed to summarize conversation: {e}")
            raise

    def get_message(self, message_id: str) -> Optional[ChatMessage]:
        """ë©”ì‹œì§€ ì¡°íšŒ"""
        return self.message_cache.get(message_id)

    def list_messages_by_session(self, session_id: str) -> List[ChatMessage]:
        """ì„¸ì…˜ë³„ ë©”ì‹œì§€ ëª©ë¡"""
        return self.conversation_history.get(session_id, [])


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_nlp_analyzer = None


async def init_nlp_analyzer() -> NLPAnalyzer:
    """NLPAnalyzer ì´ˆê¸°í™”"""
    global _nlp_analyzer

    try:
        _nlp_analyzer = NLPAnalyzer()
        logger.info("âœ… NLPAnalyzer initialized successfully")
        return _nlp_analyzer

    except Exception as e:
        logger.error(f"âŒ Failed to initialize NLPAnalyzer: {e}")
        raise


def get_nlp_analyzer() -> Optional[NLPAnalyzer]:
    """NLPAnalyzer ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return _nlp_analyzer
